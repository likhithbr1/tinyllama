import os
import sys
from typing import Optional
import logging

# Disable LangChain debug logs (if any)
logging.getLogger("langchain").setLevel(logging.ERROR)
import langchain
langchain.debug = False

# Import LangChain Community utilities and HuggingFacePipeline wrapper
from langchain_community.utilities import SQLDatabase
from langchain_huggingface import HuggingFacePipeline

from sqlalchemy import create_engine

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

def load_tinyllama_langchain_llm():
    print("‚è≥ Loading TinyLlama model (HuggingFace style)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    print("‚úÖ Model loaded!")
    hf_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,  # You can adjust sampling parameters as needed.
    )
    llm = HuggingFacePipeline(pipeline=hf_pipe)
    return llm

def pick_tables(question: str, all_tables: list) -> list:
    """Naive approach: pick tables whose names appear in the user's question.
       Fallback: pick the first 3 tables."""
    question_lower = question.lower()
    relevant = [t for t in all_tables if t.lower() in question_lower]
    return relevant or all_tables[:3]

def get_schema_text(db: SQLDatabase) -> str:
    """
    Build a textual representation of the schema from the database.
    For each table, list its columns and types.
    """
    tables = db.get_usable_table_names()
    lines = []
    for tbl in tables:
        lines.append(f"Table: {tbl}")
        try:
            # get_table_info returns a list of dicts with keys like 'name' and 'type'
            columns = db.get_table_info(tbl)
            for col in columns:
                lines.append(f"  - {col['name']} ({col['type']})")
        except Exception as e:
            # If an error occurs (e.g. reflection issue), skip that table.
            lines.append("  - [Error retrieving columns]")
        lines.append("")  # blank line between tables
    return "\n".join(lines).strip()

def generate_sql_custom(question: str, schema_text: str, llm) -> str:
    """
    Manually constructs a prompt (similar to your base version) and uses the LLM
    to generate a SQL query.
    """
    prompt = (
        "Generate an SQL query strictly based on the schema provided.\n\n"
        f"Schema:\n{schema_text}\n\n"
        f"Question:\n{question}\n\n"
        "Only output SQL code. Do not output any explanation or additional text.\n"
        "SQL:"
    )
    # Call the LLM directly; HuggingFacePipeline is callable and returns text.
    result = llm(prompt)
    return result.strip()

def main():
    # 1) Load the TinyLlama model
    llm = load_tinyllama_langchain_llm()

    # 2) MySQL credentials
    user = "root"
    password = "admin"
    host = "localhost"
    port = 3306
    database = "chatbot"
    db_uri = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"

    print("\nüîπTinyLlama Chat w/ MySQL using custom prompt (no chain-of-thought).\n")
    print("Type 'exit' or 'quit' to stop.\n")

    # 3) Build a wide DB for table discovery
    wide_db = SQLDatabase.from_uri(db_uri)
    all_table_names = wide_db.get_usable_table_names()
    # (Optional) Print table names for debugging:
    # print(f"[DEBUG] Table Names: {all_table_names}")

    while True:
        question = input("User Question: ")
        if question.strip().lower() in ["exit", "quit"]:
            break

        # 4) Choose relevant tables using partial schema selection
        relevant_tables = pick_tables(question, all_table_names)
        # (Optional) Print chosen tables for debugging:
        # print(f"[DEBUG] Using tables: {relevant_tables}")

        # 5) Reflect columns only for relevant tables
        filtered_db = SQLDatabase.from_uri(db_uri, include_tables=relevant_tables)

        # 6) Build a schema text from the filtered DB
        schema_text = get_schema_text(filtered_db)
        # (Optional) Debug print of schema:
        # print(f"[DEBUG] Schema:\n{schema_text}\n")

        # 7) Manually generate SQL using the custom prompt (like your base code)
        sql_query = generate_sql_custom(question, schema_text, llm)
        print(f"\nGenerated SQL Query:\n{sql_query}\n")

        # 8) Execute the SQL query using SQLAlchemy
        try:
            engine = create_engine(db_uri)
            with engine.connect() as connection:
                result = connection.execute(sql_query)
                rows = result.fetchall()
            print("DB Results:")
            for row in rows:
                print(row)
            print("")
        except Exception as e:
            print(f"\n‚ùå Error executing query: {e}\n")

    print("üëã Exiting. Goodbye!")

if __name__ == "__main__":
    main()
