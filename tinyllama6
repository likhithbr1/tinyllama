import os
import sys
from typing import Optional

import logging
# Disable LangChain debug logs
logging.getLogger("langchain").setLevel(logging.ERROR)

# If you have "langchain_community" installed:
# pip install -U langchain-community
from langchain_community.utilities import SQLDatabase
from langchain_community.tools import QuerySQLDatabaseTool
from langchain_huggingface import HuggingFacePipeline
from langchain.chains import create_sql_query_chain
from langchain.agents.agent_types import AgentType
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from sqlalchemy import create_engine
import langchain
langchain.debug = False  # Disable debug logging

##############################
# TINYLLAMA LOAD
##############################
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

def load_tinyllama_langchain_llm():
    print("‚è≥ Loading TinyLlama model (HuggingFace style)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    print("‚úÖ Model loaded!")
    hf_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
    )
    # Wrap pipeline in a LangChain-compatible LLM
    llm = HuggingFacePipeline(pipeline=hf_pipe)
    return llm

##############################
# PARTIAL SCHEMA SELECTION
##############################
def pick_tables(question: str, all_tables: list) -> list:
    """Naive approach: pick tables whose names appear in the question."""
    question_lower = question.lower()
    relevant = [t for t in all_tables if t.lower() in question_lower]
    # Fallback if no match
    return relevant or all_tables[:3]

def main():
    # 1) Load the TinyLlama model
    llm = load_tinyllama_langchain_llm()

    # 2) MySQL credentials
    user = "root"
    password = "admin"
    host = "localhost"
    port = 3306
    database = "chatbot"
    db_uri = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"

    print("\nüîπTinyLlama Chat w/ MySQL using LangChain.\n")
    print("Type 'exit' or 'quit' to stop.\n")

    # 3) Build a "wide" DB object for table discovery
    wide_db = SQLDatabase.from_uri(db_uri)
    all_table_names = wide_db.get_usable_table_names()
    print(f"[DEBUG] Table Names: {all_table_names}")

    while True:
        # Change: We now expect the input variable to be "input" not "question"
        user_question = input("User Question: ")
        if user_question.strip().lower() in ["exit", "quit"]:
            break

        # 4) Choose relevant tables
        relevant_tables = pick_tables(user_question, all_table_names)
        print(f"[DEBUG] Using these tables: {relevant_tables}")

        # 5) Reflect columns only for relevant tables
        filtered_db = SQLDatabase.from_uri(db_uri, include_tables=relevant_tables)

        # ===== CHANGE 1: Define a minimal custom prompt with the required placeholders =====
        custom_prompt = PromptTemplate(
            input_variables=["input", "table_info"],  # 'input' and 'table_info' will be used at runtime
            partial_variables={"top_k": "5"},         # set top_k as a fixed value (you can adjust this)
            template=(
                "Schema Information:\n{table_info}\n\n"
                "User Question:\n{input}\n\n"
                "Generate a valid MySQL query that answers the question. "
                "Return ONLY the SQL query. No extra explanation."
            )
        )
        # ====================================================================================

        # ===== CHANGE 2: Use 'input' key when invoking the chain =====
        chain = create_sql_query_chain(
            llm=llm,
            db=filtered_db,
            prompt=custom_prompt
        )

        try:
            # Pass the question using the key "input" (not "question")
            result = chain.invoke({"input": user_question})
            # Optionally, parse with StrOutputParser (if needed) to remove stray formatting:
            final_sql = StrOutputParser().parse(result)
            print(f"\nAssistant Answer:\n{final_sql}\n")
        except Exception as e:
            print(f"\n‚ùå Error while generating/executing query: {e}")

    print("üëã Exiting. Goodbye!")

if __name__ == "__main__":
    main()
